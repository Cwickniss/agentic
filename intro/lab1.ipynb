{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c18455-71ac-4bc7-8568-5f77c4b60ac1",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "# STRAIGHT TO ACTION!\n",
    "\n",
    "Welcome to our first Lab where we will see rapid, satisfying results!\n",
    "\n",
    "I will leave with you to try out leading LLMs through their Chat Interfaces\n",
    "\n",
    "Together, we will call them using their APIs\n",
    "\n",
    "Please see the README for instructions on setting this up and getting your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810279d-0b70-457d-8713-d9023a6650bf",
   "metadata": {},
   "source": [
    "# If this is your first time in a Notebook..\n",
    "\n",
    "Welcome to the world of Data Science experimentation. Warning: Jupyter Notebooks are very addictive and you may find it hard to go back to IDEs afterwards!!\n",
    "\n",
    "First click \"Select Kernel\" in the top right, per the Setup instructions, to select your uv environment.\n",
    "\n",
    "Then simply click in each cell with code and press `Shift + Enter` to execute the code and print the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24228179-fe9e-4098-8669-6898459eaa76",
   "metadata": {},
   "source": [
    "## FIRST: Calling Frontier Models through APIs\n",
    "\n",
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you'll need to create API keys from OpenAI, Anthropic and Google.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables.\n",
    "\n",
    "Then create a file called `.env` in this project root directory, and set your keys there:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3506e-b46f-44f0-ba9b-6b002835d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import requests\n",
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae0a99-0235-4256-9874-1b5e718b6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c0d08-2ecb-4fd2-aa22-c6ed64334952",
   "metadata": {},
   "source": [
    "## Connecting to Python Client libraries\n",
    "\n",
    "We call Cloud APIs by making REST calls to an HTTP endpoint, passing in our keys.\n",
    "\n",
    "For convenience, the labs like OpenAI have provided lightweight python client libraries that make the HTTP calls for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c6483-a9a6-4e3b-b361-e7a8c7f912e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to REST endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For other providers, we can use the OpenAI python client\n",
    "# Because they have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584134a-68c6-4677-b4b0-1a7c1c93aeb4",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A **system message** that gives overall context for the role the LLM is playing\n",
    "- A **user prompt** that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic.\n",
    "\n",
    "### The standard format of messages with an LLM, first used by OpenAI in its API and now adopted more widely\n",
    "\n",
    "Conversations use this format:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbcc5b-8517-4a3e-9ca1-e860a072bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes that are topical and relevant for now\"\n",
    "user_prompt = \"Tell a light-hearted joke that's related to Agentic AI and Autonomous Agents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead0d09-cf0f-443c-b2ce-ff1a4ecdf49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cd59d-3efe-41e7-b149-ce56b9697a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-5-mini\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=tell_a_joke)\n",
    "result = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836cb8e-3d40-4cbd-98a5-8aa6764bc522",
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle = [{\"role\": \"user\", \"content\": \"I flip 2 coins. One of them is heads. What's the chance the other is tails? Give the answer only.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe5aaa",
   "metadata": {},
   "source": [
    "## Training vs Inference time scaling\n",
    "\n",
    "**Training time scaling**: use a larger model with more parameters trained on more data\n",
    "\n",
    "Like `gpt-5-nano` -> `gpt-5-mini`\n",
    "\n",
    "**Inference time scaling**: spend more time generating tokens to get to better outcomes (like, \"think step by step\")\n",
    "\n",
    "Like reasoning effort of `minimal` -> reasoning effort of `low`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e812d-f194-4a16-b3e5-fa07b9e586c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=puzzle, reasoning_effort=\"minimal\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4e048-98e3-4f4e-9bd6-d8b318b72f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a joke from the best model on the planet\n",
    "\n",
    "response = openai.chat.completions.create(model='gpt-5',messages=tell_a_joke)\n",
    "result = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66500e7d-6913-4703-abf8-784c7aa210cf",
   "metadata": {},
   "source": [
    "## Let's mix it up - the \"Prisoner's Dilemma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e450f83-a76c-499f-bc40-028ec2301966",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "\n",
    "Cooperate: Choose \"Share\" — if both of you choose this, you each win $1,000.\n",
    "\n",
    "Defect: Choose \"Steal\" — if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "\n",
    "If both steal, you both get nothing.\n",
    "\n",
    "Do you choose to Steal or Share? Pick one. Be concise.\n",
    "\"\"\"\n",
    "\n",
    "decision = [{\"role\": \"user\", \"content\": decision_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbb3fe-6c9e-4f1a-ae16-c20445fc5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4 Sonnet\n",
    "\n",
    "response = anthropic.chat.completions.create(model='claude-sonnet-4-5',messages=decision)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2fa42",
   "metadata": {},
   "source": [
    "## Anthropic's models come in 3 sizes:\n",
    "\n",
    "Haiku: small  \n",
    "Sonnet: medium  \n",
    "Opus: massive\n",
    "\n",
    "Haiku is 3 times cheaper than Sonnet but in many cases gives similar answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model='claude-haiku-4-5',messages=decision)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543cc49-fe01-4a6c-af4e-c576f4bbc06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini 2.5 Pro - Gemini 3 is coming very soon!\n",
    "\n",
    "response = gemini.chat.completions.create(model='gemini-2.5-pro', messages=decision)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b22d4-1623-4387-bbaa-ab4a559b9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek v3.2\n",
    "\n",
    "response = deepseek.chat.completions.create(model='deepseek-chat',messages=decision)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df7f787-f6b4-4648-b235-32a811dd5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grok4\n",
    "\n",
    "response = grok.chat.completions.create(model='grok-4',messages=decision)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7725677",
   "metadata": {},
   "source": [
    "## Using LiteLLM - a wonderful abstraction layer\n",
    "\n",
    "1. Makes it easy to switch between models\n",
    "2. Provides a simple API\n",
    "3. Also calculates tokens and costs - very handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92127027-8091-4f61-955e-9b067036ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LiteLLM - a wonderfully simple abstraction layer\n",
    "\n",
    "response = completion(\"openai/gpt-4.1-nano\", messages=tell_a_joke)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df2b7d4-99ed-4bc1-8f8a-3b873d52b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is so useful!\n",
    "\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6b3c0",
   "metadata": {},
   "source": [
    "## More advanced topic: prompt caching & LiteLLM budgeting\n",
    "\n",
    "### Let's load in the entire play \"Hamlet\" and look in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e894589",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = hamlet_text.find(\"Where is my father?\")\n",
    "print(hamlet_text[location-50:location+50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec186fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_without_hamlet = \"\"\"\n",
    "You are an assistant that can answer questions about the text of Hamlet.\n",
    "\"\"\"\n",
    "question = \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"\n",
    "hamlet = [{\"role\": \"system\", \"content\": system_without_hamlet}, {\"role\": \"user\", \"content\": question}]\n",
    "response = completion(\"openai/gpt-4.1-nano\", messages=hamlet)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e51c7-9991-44f9-894b-1a8a98eea257",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "system_with_hamlet = f\"\"\"\n",
    "You are a helpful assistant that can answer questions about the text of Hamlet.\n",
    "For context, here is the entire text of Hamlet:\n",
    "{hamlet_text}\n",
    "\"\"\"\n",
    "question = \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"\n",
    "hamlet = [{\"role\": \"system\", \"content\": system_with_hamlet}, {\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8565b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(\"openai/gpt-4.1-nano\", messages=hamlet)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca76703-386d-4a4f-8897-6437cebed41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62be26",
   "metadata": {},
   "source": [
    "I got this:\n",
    "\n",
    "> When Laertes asks \"Where is my father?\" in Hamlet, the reply is \"Dead.\"  \n",
    "> Input tokens: 49706  \n",
    "> Output tokens: 19  \n",
    "> Cached tokens: 0  \n",
    "> Total cost: 0.4978 cents  \n",
    "\n",
    "Now let's try again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde21d5c-65e0-403c-81c1-c4d382907ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(\"openai/gpt-4.1-nano\", messages=hamlet)\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48e307",
   "metadata": {},
   "source": [
    "### Now let's use Groq (fast inference) in the cloud with GPT-OSS-120B, the new Open Source model from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e4982-c3af-48fd-8ce9-b44a86898800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! Groq (fast inference on the cloud) with a proper question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a knowledgable assistant, and you respond in markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the commercial applications of LLMs? Please respond in markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08a70a-f0d8-4317-8690-c12933e0d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = groq.chat.completions.create(\n",
    "    model='openai/gpt-oss-120b',\n",
    "    messages=prompts,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e38c6-a5db-4e04-99bd-4a7a0031aa97",
   "metadata": {},
   "source": [
    "# Now for Part 3\n",
    "\n",
    "### Recap: first we tried Frontier LLMs through their chat interfaces\n",
    "\n",
    "### Then we called Cloud APIs\n",
    "\n",
    "### And now:\n",
    "\n",
    "Now try the 3rd way to use LLMs - direct inference of Open Source Models running locally with Ollama  \n",
    "\n",
    "Visit the README for instructions on installing Ollama locally.\n",
    "\n",
    "You can see some comparisons of Open Source models on the HuggingFace OpenLLM Leaderboard.\n",
    "\n",
    "Ollama provides an OpenAI-style local endpoint, so this will look very similar to part 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6258c1d-c83d-421d-a1fa-905d675d16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull gemma3:270b\n",
    "!ollama pull gpt-oss:20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bc8a4-c7b5-4934-b330-b9a9a41f4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_url = 'http://localhost:11434/v1'\n",
    "ollama = OpenAI(base_url=ollama_url, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b1961-95fa-487b-826f-66b1a27e76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a47397-1938-4351-9bd9-df7a06688ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemma3:270b\n",
    "\n",
    "model_name = \"gemma3:270m\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=tell_a_joke)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08880fb-1000-4678-abb4-f48377ae30da",
   "metadata": {},
   "source": [
    "## The illusion of memory\n",
    "\n",
    "Each call to the OpenAI API is stateless; GPT has no knowledge of the prior message, even if it was seconds ago.\n",
    "\n",
    "So how is it possible to hold a conversation with GPT and keep context? It's a trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9d9a2-3316-464f-80b6-207c0330eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\":\"user\", \"content\": \"Hello, my name is Ed\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2dddf-f189-4289-bf99-503f8a887547",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\":\"user\", \"content\": \"What's my name\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31fd9f-1653-443b-a767-a5efa5805118",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\":\"user\", \"content\": \"Hello, my name is Ed\"},\n",
    "    {\"role\":\"assistant\", \"content\": \"Hello Ed! How can I assist you today?\"},\n",
    "    {\"role\":\"user\", \"content\": \"What's my name\"}\n",
    "]\n",
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7a0e3-785b-403d-91a1-d28ea8297943",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08b981-f27e-4e50-aaa7-6ab1c0e09f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-nano and Claude-haiku-4-5\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-nano\"\n",
    "claude_model = \"claude-haiku-4-5\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557a7c8-b574-4394-a6ca-97d49a45d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b352f82-a94e-43b5-a4c7-71c6eeb5d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(call_gpt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9cc43-e9b7-4179-b318-1e60998665bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23ebf0-2816-4953-94ec-a96b931179d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16d1f1-ea9c-45c2-9228-7f63d1b00a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c88f8-9b16-475e-9f98-a3c7a2a1caed",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "This was an entertaining exercise!\n",
    "\n",
    "At the same time, it hopefully gave you some perspective on:\n",
    "- The use of system prompts to set tone and character\n",
    "- The way that the entire conversation history is passed in to each API call, giving the illusion that LLMs have memory of the chat so far\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Try different characters; try swapping Claude with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2b8e3-182b-430b-9c89-d5e89f32a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And just to show you how easy it is: let's generate an image\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import base64\n",
    "\n",
    "response = openai.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=\"A photorealistic 3d image that represents the power of a Frontier LLM in solving real business use cases\",\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"standard\",\n",
    "  n=1,\n",
    "  response_format=\"b64_json\"\n",
    ")\n",
    "\n",
    "image_base64 = response.data[0].b64_json\n",
    "image_data = base64.b64decode(image_base64)\n",
    "display(Image(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ead89-a107-429a-be0b-c61eb7264129",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=\"A vibrant, pop-art style image that represents the power of a Frontier LLM in solving real business use cases\",\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"standard\",\n",
    "  n=1,\n",
    "  response_format=\"b64_json\"\n",
    ")\n",
    "\n",
    "image_base64 = response.data[0].b64_json\n",
    "image_data = base64.b64decode(image_base64)\n",
    "display(Image(image_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
